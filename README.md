## Nostalgic Adam
### repository for the paper: Nostalgic Adam: Weighting more of the past gradients when designing the adaptive learning rate
Haiwen Huang, Chang Wang, Bin Dong (https://arxiv.org/abs/1805.07557) 

Dependencies: Python >= 3.5, Pytorch >= 0.4.0

An introduction to the paper in Chinese: https://zhuanlan.zhihu.com/p/65625686

If you find this code useful, please cite:
```
@article{DBLP:journals/corr/abs-1805-07557,
  author    = {Haiwen Huang and
               Chang Wang and
               Bin Dong},
  title     = {Nostalgic Adam: Weighing more of the past gradients when designing
               the adaptive learning rate},
  journal   = {CoRR},
  volume    = {abs/1805.07557},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.07557},
  archivePrefix = {arXiv},
  eprint    = {1805.07557},
  timestamp = {Mon, 13 Aug 2018 16:47:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-07557},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

